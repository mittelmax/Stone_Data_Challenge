---
title: "Stone Data Challenge - Modelagem"
author: "Por Max Mitteldorf"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: united
toc-title: "Índice"
---
```{r setup, include=FALSE}
options(scipen = 999)
require("knitr")
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
opts_knit$set(root.dir = "/Users/maxmitteldorf/Desktop/stone_data_envio")
```

```{r, echo=F, eval=T}
library(reticulate)
# py_install("pandas")
# py_install("scikit-learn")
# py_install("numpy")
```

## Feature Engineering
Minha estratégia de feature engineering foi bastante convencional. Meu método foi aplicar uma série de lags e médias móveis a todas as variáveis, a fim de evitar data leakage entre os períodos.
\
Vale comenntar que a partir desse momento continuei o desafio com Python.
\
Primeiramente podemos importar os pacotes e os dataframes utilizados no script. Nesse markdown utilizarei apenas as primeiras 1000 linhas do dataframe por ser apenas uma demonstração dos métodos aplicados:
```{python, eval=T}
from stone_data_challenge.config import data_dir
from sklearn.impute import SimpleImputer
import datetime as dt
import numpy as np
import datetime
import pandas as pd

# # Reading dataframe
pd.set_option('display.max_columns', 10)
pd.set_option('display.width', 1000)
df = pd.read_csv(f"{data_dir}/data/clean/spine_stone_ipea.csv")
df = df.head(100000)
df = df.replace("nan", np.NaN)
df = df.drop_duplicates()
df.head(5)
```
\
Primeiramente vamos checar os tipos das variáveis para ver se podemos armazená-las em algum tipo de dado mais eficiente:
```{python, eval=T}
df.dtypes
```
\
Vamos agora corrigir os tipos de algumas variáveis:
```{python, eval=T}
# # Fixing types
df[["MCC", "MacroClassificacao", "StoneCreatedDate", "StoneFirstTransactionDate", "segmento",
    "sub_segmento", "persona", "tipo_documento", "Estado", "mes_referencia", "id"]] = df[["MCC", "MacroClassificacao", "StoneCreatedDate", "StoneFirstTransactionDate", "segmento", "sub_segmento", "persona", "tipo_documento", "Estado", "mes_referencia", "id"]].astype(str)
df.dtypes
```
\
Agora podemos consertar o formato das datas:
```{python, eval=T}
# # Fixing date formatting
df["StoneCreatedDate"] = pd.to_datetime(df["StoneCreatedDate"]).dt.tz_localize(None)
df["StoneFirstTransactionDate"] = pd.to_datetime(df["StoneFirstTransactionDate"])
df["mes_referencia"] = pd.to_datetime(df["mes_referencia"])
df[["StoneFirstTransactionDate", "StoneCreatedDate"]].head(5)
```
\
Algo que podemos fazer para o modelo é transformar as variáveis sobre a data de cadastro do cliente e sua primeira transação em numeros ordinais Observe:
```{python, eval=T}
# # Transforming date variables in cardinal numbers
initial_date = dt.datetime(2020, 7, 31)
df["DaysSinceCreation"] = -(df["StoneCreatedDate"] - initial_date).dt.days
df["DaysSinceFirstTrans"] = -(df["StoneFirstTransactionDate"] - initial_date).dt.days
df[["DaysSinceCreation", "DaysSinceFirstTrans"]]
```

\
Para que nosso modelo possa identificar os efeitos de sazonalidade é interessante transformar a dividir a coluna "mes_referencia" em duas variáveis diferentes: uma para mês e outra para ano. Apliquei o mesmo tratamento para "StoneCreatedDate" e "StoneFirstTransactionDate".
```{python, eval=T}
# # Creating separate year and month variables for date imputing
df["YearCreated"] = df["StoneCreatedDate"].dt.year
df["MonthCreated"] = df["StoneCreatedDate"].dt.month
df["YearFirstTrans"] = df["StoneFirstTransactionDate"].dt.year
df["MonthFirstTrans"] = df["StoneFirstTransactionDate"].dt.month
df["AnoReferencia"] = df["mes_referencia"].dt.year
df["MesReferencia"] = df["mes_referencia"].dt.month

df[["YearCreated", "MonthCreated", "YearFirstTrans", "MonthFirstTrans", "AnoReferencia", "MesReferencia"]]
```
\
Criadas essas variáveis agora podemos remover as variáveis de data não ordinais, com excessão de "mes_referencia" que será utilizada como índice mais adiante:
```{python, eval=T}
# # # Removing unwanted variables
df.drop(["mes", "StoneFirstTransactionDate", "StoneCreatedDate"], axis="columns", inplace=True)
```
\
Vamos checar agora a porcentagem de valores faltantes em cada coluna no dataframe:
```{python, eval=T}
# # Checking for missing data
df.replace('nan', np.nan, inplace=True)
percent_missing = df.isna().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({"col_name": df.columns, "pct_missing": percent_missing})
missing_value_df
```
\
Primeiramente podemos imputar alguns valores categóricos como "Desconhecido" para as colunas referentes a o setor dos clientes:
```{python, eval=T}
# # Checking for missing data
df["MacroClassificacao"] = df["MacroClassificacao"].fillna("Desconhecido")
df["segmento"] = df["segmento"].fillna("Desconhecido")
df["sub_segmento"] = df["sub_segmento"].fillna("Desconhecido")
```
\
Da análise exploratória sabemos que São Paulo é de longe o estado com maior número de clientes Stone. Dessa forma podemos imputar os valores faltantes com São Paulo:
```{python, eval=T}
# # Imputing missing categorical data for state
df["Estado"] = df["Estado"].fillna("SP")
```
\
Por último sobraram os valores faltantes para as colunas "ind_varejo", "demissoes", "DaysSinceFirstTrans", "YearFirstTrans"e "MonthFirstTrans".
Para imputar os valores nulo utilizarei o imputador do scikit-learn e substituirei os valores faltantes pela mediana:
```{python, eval=T}
# Creating column transformer
missing_vars = ["ind_varejo", "demissoes", "DaysSinceFirstTrans", "YearFirstTrans", "MonthFirstTrans"]
imputer = SimpleImputer(strategy="median")
df[missing_vars] = imputer.fit_transform(df[missing_vars])  # Imputing data
```
\
Agora podemos conferir que não há mais valores nulos:
```{python, eval=T}
# # Checking for missing data
percent_missing = df.isnull().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({"col_name": df.columns, "pct_missing": percent_missing})
missing_value_df
```
### Geração de variáveis com lag e médias móveis
Agora chegamos em um momento muito importante. Pela forte característica autoregressiva de nosso problema devemos gerar uma série de variáveis com defasagem temporal de 1 e mais períodos. Também é interessante gerar médias móveis para distintas janelas de tempo.
\
\
Antes de começar a adicionar variáveis devemos adicionar 5 meses à base de dados, para que possamos utilizar essas variáveis com lag e médias móveis na previsão final do desfafio. Esse é o código que utilizei para isso:
```{python, eval=T}
df = df.set_index(["mes_referencia"])

# # We need to add 5 months to the dataset before shifting the columns
df_agosto = df.groupby("id").tail(1)
df_agosto.index = df_agosto.index + datetime.timedelta(days=30)
df_agosto.loc[:, "TPV_mensal"] = np.nan
df_agosto.loc[:, "caixas_papelao"] = np.nan
df_agosto.loc[:, "caixas_papelao_varpct"] = np.nan
df_agosto.loc[:, "ipca_pct"] = np.nan
df_agosto.loc[:, "cambio_dol"] = np.nan
df_agosto.loc[:, "demissoes"] = np.nan
df_agosto.loc[:, "ind_varejo"] = np.nan
df_agosto[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_agosto[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_agosto["MesReferencia"] = df_agosto["MesReferencia"] + 1

df_setembro = df_agosto[:]
df_setembro.index = df_setembro.index + datetime.timedelta(days=30)
df_setembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_setembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_setembro["MesReferencia"] = df_setembro["MesReferencia"] + 1

df_outubro = df_setembro[:]
df_outubro.index = df_outubro.index + datetime.timedelta(days=30)
df_outubro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_outubro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_outubro["MesReferencia"] = df_outubro["MesReferencia"] + 1

df_novembro = df_outubro[:]
df_novembro.index = df_novembro.index + datetime.timedelta(days=30)
df_novembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_novembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_novembro["MesReferencia"] = df_novembro["MesReferencia"] + 1

df_dezembro = df_novembro[:]
df_dezembro.index = df_dezembro.index + datetime.timedelta(days=30)
df_dezembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_dezembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_dezembro["MesReferencia"] = df_dezembro["MesReferencia"] + 1

df = pd.concat([df, df_agosto,df_setembro,df_outubro,df_novembro,df_dezembro], axis=0)
df = df.sort_values(["id", "mes_referencia"])
```
\
Agora podemos conferir que temos mais 5 meses na base de dados:
```{python, eval=T, echo=T}
df.tail(8)
```
\
\
Vamos agora começar a adicionar as médias móveis e variáveis com lag. Vou exemplificar o tratamento que conferi ao TPV mensal. Repeti exatamente o mesmo procedimento as outras variáveis:
```{python, eval=T, echo=T}
# Lag TPV
df["TPV_lag_1"] = df.groupby("id")["TPV_mensal"].shift(1)
df["TPV_lag_2"] = df.groupby("id")["TPV_mensal"].shift(2)
df["TPV_lag_3"] = df.groupby("id")["TPV_mensal"].shift(3)
df["TPV_lag_4"] = df.groupby("id")["TPV_mensal"].shift(4)
df["TPV_lag_5"] = df.groupby("id")["TPV_mensal"].shift(5)
df["TPV_lag_6"] = df.groupby("id")["TPV_mensal"].shift(6)
df["TPV_lag_12"] = df.groupby("id")["TPV_mensal"].shift(12)

# Creating moving averages for TPV
df["TPV_MA_2"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(2, 2).mean())
df["TPV_MA_3"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(3, 2).mean())
df["TPV_MA_4"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(4, 2).mean())
df["TPV_MA_5"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(5, 2).mean())
df["TPV_MA_6"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(6, 2).mean())
df["TPV_MA_12"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(12, 2).mean())
df["TPV_MA_18"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(18, 2).mean())
```
\
Vamos verificar que essas transformações funcionaram:
```{python, eval=T, echo=T}
df[["TPV_mensal","TPV_lag_1","TPV_lag_3","TPV_lag_3","TPV_lag_4","TPV_MA_2","TPV_MA_3","TPV_MA_4"]].tail(10)
```
\
Por último, após aplicar tal procedimento em todas as variáveis podemos salvar o dataframe final para a modelagem:
```{python, eval=F, echo=T}
# Writing dataframe
df = df.reset_index()
df.to_csv(f"{data_dir}/feat_eng/model_data_v5.csv", index=False)
```

## Modelagem
Diante de um problema com um componente temporal e que exige a previsão de 5 lags para frente, optei por construir 5 modelos independentes (um para cada lag temporal).
\
\
Decidi por utilizar o [LightGBM](https://lightgbm.readthedocs.io/en/latest/) para fazer as previsões por sua rapidez e robustez. Diante um grande número de hiperparâmetros também optei por utilizar o pacote [Optuna](https://optuna.org/) para otimizá-los.
\
\
Agora mostrarei o código para o modelo que prevê um lag temporal (t+1). O código para os modelos (t+2) em diante são muito similares e estão disponíveis no repositório.
\
\
Primeiramente vamos importar os pacotes necessários:
```{python, eval=F, echo=T}
import pandas as pd
import numpy as np
from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples
from sklearn.utils.validation import _deprecate_positional_args
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import optuna
import pickle
```
\
Em seguida as configurações do script devem ser definidas:
* `TUNING` - Indica se a otimização dos parâmetros será realizada
* `TUNING_SIZE` - Indica o tamanho do dataframe para a otimização dos parâmetros
* `TRAINING` - Indica se o modelo será treinado ou não
* `TRAINING_SIZE` - Indica o tamanho do dataframe para o treino do modelo
* `FINNAL_PREDICTION` - Indica se queremos treinar o modelo em todos os dados e exportar a previsão final do desafio
\
\
Agora podemos começar lendo o dataframe e removendo as colunas sem lag temporal para evitarmos problemas de data leakage temporal:
```{python, eval=F, echo=T}
# # Reading dataframe
print("Reading Dataframe")
df = pd.read_csv(f"{data_dir}/data/feat_eng/model_data_v5.csv")
print("Dataframe succesfully readed")

# # Dropping columns without lag e MA2
df.drop(["caixas_papelao", "caixas_papelao_varpct", "ipca_pct", "cambio_dol", "demissoes", "ind_varejo",
         "TPV_MA_2", "ipca_MA_2", "caixas_papelao_MA_2", "caixas_papelao_varpct_MA_2",
         "demissoes_MA_2", "ind_varejo_MA_2", "cambio_dol_MA_2"], axis="columns", inplace=True)


# # Renaming TPV columns
df.rename(columns={"TPV_mensal": "TPV_t1"}, inplace=True)
```
\
Em seguida usei o LabelEncoder() para transformar as variáveis discretas em categóricas. Utilizei o LabelEncoder() no lugar do OneHotEcoder() porque modelos de árvore como o LightGBM lidam bem com tal formato de dados.
```{python, eval=F, echo=T}
# # Using LabelEncoder for categorical variables
# # Lightgbm can handle label encoded variables very efficiently
le = LabelEncoder()
df["MCC"] = le.fit_transform(df["MCC"])
df["MacroClassificacao"] = le.fit_transform(df["MacroClassificacao"])
df["segmento"] = le.fit_transform(df["segmento"])
df["sub_segmento"] = le.fit_transform(df["sub_segmento"])
df["persona"] = le.fit_transform(df["persona"])
df["Estado"] = le.fit_transform(df["Estado"])
df["tipo_documento"] = le.fit_transform(df["tipo_documento"])
```
\
Agora também podemos separar os meses da previsão final (agosto até dezembro) da base de dados e separá-los para um momento posterior:
```{python, eval=F, echo=T}
# # Saving dataset for prediction
df["mes_referencia"] = pd.to_datetime(df["mes_referencia"])
df_prediction = df.groupby("id").tail(5).set_index("mes_referencia")
df_prediction_august = df_prediction.groupby("id").head(1).drop("TPV_t1", axis="columns")
df_prediction_august["ind"] = df_prediction_august["id"]
df_prediction_august = df_prediction_august.set_index("ind")
df = df[df["mes_referencia"] < "2020-08-30"]
```
\
Agora, como desejamos poder avaliar as métricas de performance do modelo, devemos remover os próximos 5 meses de dados para deixá-los como base de validação final. Também tive que fazer algumas manipulações para remover os dados de algumas variáveis regressoras que não deveriam estar dsiponíveis no momento da execuão do modelo.
```{python, eval=F, echo=T}
df_val = df.groupby("id").tail(5).groupby("id").head(1)
df_val = df_val[pd.notnull(df_val["TPV_t1"])].set_index("mes_referencia")

df_train = df.drop(df.groupby("id").tail(5).index, axis=0).set_index("mes_referencia")
df_train = df_train[pd.notnull(df_train["TPV_t1"])]
df_train = df_train.head(TRAINING_SIZE)

df = df.set_index("mes_referencia")
df = df[pd.notnull(df["TPV_t1"])]

# # Saving column names for later
colnames = df.columns

# # Saving categorical vars names
x_colnames = list(df.drop("TPV_t1", axis="columns").columns.values)
categorical_vars = ["id", "MCC", "MacroClassificacao", "segmento", "sub_segmento", "persona", "tipo_documento", "Estado"]
```
\
Agora gostaria de falar um pouco sobre minha estratégia para separar as bases de treino e teste. Estamos diante de um problema de séries temporais, por isso a princípio poderíamos utilizar o TimeSerisSplit() para treinar nosso modelo em vários ranges de tempo sem que ocorra time leakage. A questão é que osso problema é de séries temporais com grupos (nesse caso o id dos clientes). Dessa forma o TimeSeriesSplit() poderia misturar clientes com ids diferentes, o que não queremos de forma alguma.
\
\
É aí que entra o GroupTimeSeriesSplit(), uma estratégia de divisão que respeita respeita a temporalidade como o grupo das observações. Essa funcionalidade ainda não está completamente implementada no scikit-learn, mas está em vias de ser. Dessa forma conseguimos acessar o código para ela nesse [link](https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243).
```{python, eval=F, echo=T}
class GroupTimeSeriesSplit(_BaseKFold):
    """Time Series cross-validator variant with non-overlapping groups.
    Provides train/test indices to split time series data samples
    that are observed at fixed time intervals according to a
    third-party provided group.
    In each split, test indices must be higher than before, and thus shuffling
    in cross validator is inappropriate.
    This cross-validation object is a variation of :class:`KFold`.
    In the kth split, it returns first k folds as train set and the
    (k+1)th fold as test set.
    The same group will not appear in two different folds (the number of
    distinct groups has to be at least equal to the number of folds).
    Note that unlike standard cross-validation methods, successive
    training sets are supersets of those that come before them.
    Read more in the :ref:`User Guide <cross_validation>`.
    Parameters
    ----------
    n_splits : int, default=5
        Number of splits. Must be at least 2.
    max_train_size : int, default=None
        Maximum size for a single training set.
    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.model_selection import GroupTimeSeriesSplit
    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\
                           'b', 'b', 'b', 'b', 'b',\
                           'c', 'c', 'c', 'c',\
                           'd', 'd', 'd'])
    >>> gtss = GroupTimeSeriesSplit(n_splits=3)
    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):
    ...     print("TRAIN:", train_idx, "TEST:", test_idx)
    ...     print("TRAIN GROUP:", groups[train_idx],\
                  "TEST GROUP:", groups[test_idx])
    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\
    TEST GROUP: ['b' 'b' 'b' 'b' 'b']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\
    TEST GROUP: ['c' 'c' 'c' 'c']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\
    TEST: [15, 16, 17]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\
    TEST GROUP: ['d' 'd' 'd']
    """

    @_deprecate_positional_args
    def __init__(self,
                 n_splits=5,
                 *,
                 max_train_size=None
                 ):
        super().__init__(n_splits, shuffle=False, random_state=None)
        self.max_train_size = max_train_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set.
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples
            and n_features is the number of features.
        y : array-like of shape (n_samples,)
            Always ignored, exists for compatibility.
        groups : array-like of shape (n_samples,)
            Group labels for the samples used while splitting the dataset into
            train/test set.
        Yields
        ------
        train : ndarray
            The training set indices for that split.
        test : ndarray
            The testing set indices for that split.
        """
        if groups is None:
            raise ValueError(
                "The 'groups' parameter should not be None")
        X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        group_dict = {}
        u, ind = np.unique(groups, return_index=True)
        unique_groups = u[np.argsort(ind)]
        n_samples = _num_samples(X)
        n_groups = _num_samples(unique_groups)
        for idx in np.arange(n_samples):
            if (groups[idx] in group_dict):
                group_dict[groups[idx]].append(idx)
            else:
                group_dict[groups[idx]] = [idx]
        if n_folds > n_groups:
            raise ValueError(
                ("Cannot have number of folds={0} greater than"
                 " the number of groups={1}").format(n_folds,
                                                     n_groups))
        group_test_size = n_groups // n_folds
        group_test_starts = range(n_groups - n_splits * group_test_size,
                                  n_groups, group_test_size)
        for group_test_start in group_test_starts:
            train_array = []
            test_array = []
            for train_group_idx in unique_groups[:group_test_start]:
                train_array_tmp = group_dict[train_group_idx]
                train_array = np.sort(np.unique(
                    np.concatenate((train_array,
                                    train_array_tmp)),
                    axis=None), axis=None)
            train_end = train_array.size
            if self.max_train_size and self.max_train_size < train_end:
                train_array = train_array[train_end -
                                          self.max_train_size:train_end]
            for test_group_idx in unique_groups[group_test_start:
            group_test_start +
            group_test_size]:
                test_array_tmp = group_dict[test_group_idx]
                test_array = np.sort(np.unique(
                    np.concatenate((test_array,
                                    test_array_tmp)),
                    axis=None), axis=None)
            yield [int(i) for i in train_array], [int(i) for i in test_array]
```
\
A partir daqui podemos definir uma função de otimização para o Optuna e realizar o tuning dos parâmetros do LightGBM em uma porção do dataframe. Após a otimização os melhores parâmetros são salvados em um dicionário através de um pickle.
```{python, eval=F, echo=T}
if TUNING:
    df_train_tuning = df_train.head(TUNING_SIZE)

    # # Objective Function
    def objective(trial, cv_fold_func=np.average):
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'boosting': 'rf',
            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
            'num_leaves': trial.suggest_int('num_leaves', 2, 200),
            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10)}

        # fitting and returning RMSE scores
        mae_list = []
        for train_idx, test_idx in GroupTimeSeriesSplit().split(df_train_tuning, groups=df_train_tuning["id"]):
            train_x, test_x = df_train_tuning.drop("TPV_t1", axis="columns").values[train_idx], df_train_tuning.drop("TPV_t1", axis="columns").values[test_idx]
            train_y, test_y = df_train_tuning["TPV_t1"][train_idx], df_train_tuning["TPV_t1"][test_idx]

            train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)

            model = lgb.train(params, train_data)
            pred = model.predict(test_x)
            mae = mean_absolute_error(pred, test_y)
            mae_list.append(mae)

        print("Trial done: MAE values on folds: {}".format(mae_list))
        return cv_fold_func(mae_list)


    # # Optuna results
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=80)
    print("Number of finished trials:", len(study.trials))
    print("Best trial:", study.best_trial.params)
    best_params = study.best_trial.params

    # Building parameter dictionary
    fixed_params = {'objective': 'regression',
                    'metric': 'mae',
                    'boosting': 'gbdt',
                    'num_threads': 4}

    best_params = {**best_params, **fixed_params}

    # # Saving best params
    filename = f"{data_dir}/data/model/parameters/best_params_t1.pickle"
    outfile = open(filename, "wb")
    pickle.dump(best_params, outfile)
    outfile.close()

else:
    # # Reading optimal parameters
    infile = open(f"{data_dir}/data/model/parameters/best_params_t1.pickle", "rb")
    best_params = pickle.load(infile)
```
\
Agora, com uma boa seleção de parâmetros, podemos partir para o treino do LightGBM com o objetivo de avaliar a performance nos dados que deixamos separados. Depois disso salvamos em pickles o erro absoluto médio do modelo, a importância das variáveis e as previsões no base de avaliação:
```{python, eval=F, echo=T}
if TRAINING:
    # # Training model
    print("Splitting train and test sets...")
    for train_idx, test_idx in GroupTimeSeriesSplit().split(df_train, groups=df_train["id"]):
        train_x, test_x = df_train.drop("TPV_t1", axis="columns").values[train_idx], df_train.drop("TPV_t1", axis="columns").values[test_idx]
        train_y, test_y = df_train["TPV_t1"][train_idx], df_train["TPV_t1"][test_idx]
    print("Beginning training...")
    train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    test_data = lgb.Dataset(test_x, label=test_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    lgbm_model = lgb.train(best_params, train_data, 250, valid_sets=test_data, early_stopping_rounds=50, verbose_eval=10)

    # # Predicting on Test Data
    # Creating validation sets for each month
    df_val_t1_y = df_val["TPV_t1"]
    df_val_t1_x = df_val.drop(["TPV_t1"], axis="columns")
    y_pred_lgb_test = lgbm_model.predict(df_val_t1_x, num_iteration=lgbm_model.best_iteration)

    # # Model performance on test data
    MAE_lgb_test = mean_absolute_error(y_pred_lgb_test, df_val_t1_y)

    # # Feature importances
    imp = pd.DataFrame(lgbm_model.feature_importance()).T
    imp.columns = x_colnames
    imp = imp.T
    imp.columns = ["feat_importance"]
    imp = imp.sort_values("feat_importance")

    # Saving Results
    imp.to_csv(f"{data_dir}/data/model/performance/importances_t1.csv")

    # Saving Results
    filename = f"{data_dir}/data/model/performance/MAE_t1.pickle"
    outfile = open(filename, "wb")
    pickle.dump(MAE_lgb_test, outfile)
    outfile.close()

    # # Saving test prediction

    # Joining test set and predictions
    df_y_pred = pd.DataFrame(y_pred_lgb_test, columns=["TPV_pred_t1"])
    df_val_t1_x = df_val_t1_x.reset_index()
    df_val_t1_y = pd.DataFrame(df_val_t1_y)
    df_val_t1_y = df_val_t1_y.reset_index().drop("mes_referencia", axis="columns")
    df_pred_t1 = pd.concat([df_val_t1_x, df_val_t1_y, df_y_pred], axis="columns").set_index("mes_referencia")
    df_pred_t1 = df_pred_t1[["id", "TPV_t1", "TPV_pred_t1"]]

    # Saving dataframe
    df_pred_t1.to_csv(f"{data_dir}/data/performance/test_pred_t1.csv")
```
\
Por último após termos decidido que estamos satisfeitos com o modelo podemos treinar o LightGBM em todos os dados e realizar a previsão finnal do desafio para o mês de Agosto:
```{python, eval=F, echo=T}
if FINAL_PREDICTION:
    print("Training on all data...")
    # # Final Prediction
    print("Splitting train and test sets...")
    for train_idx, test_idx in GroupTimeSeriesSplit().split(df, groups=df["id"]):
        train_x, test_x = df.drop("TPV_t1", axis="columns").values[train_idx], df.drop("TPV_t1", axis="columns").values[test_idx]
        train_y, test_y = df["TPV_t1"][train_idx], df["TPV_t1"][test_idx]
    print("Beginning training...")
    train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    test_data = lgb.Dataset(test_x, label=test_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    lgbm_model = lgb.train(best_params, train_data, 250, valid_sets=test_data, early_stopping_rounds=50, verbose_eval=10)

    # # Predicting on Test Data
    # Creating validation sets for each month
    y_pred_lgb_final = lgbm_model.predict(df_prediction_august, num_iteration=lgbm_model.best_iteration)

    # # Exporting final results
    y_pred_lgb_final = pd.DataFrame(y_pred_lgb_final, columns=["TPV Agosto"])
    y_pred_lgb_final.to_csv(f"{data_dir}/data/predictions/prediction_august.csv")
```


