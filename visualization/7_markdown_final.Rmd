---
title: "Stone Data Challenge - Previsão de TPV"
author: "Por Max Mitteldorf"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: united
toc-title: "Índice"
---
```{r setup, include=FALSE}
options(scipen = 999)
require("knitr")
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
opts_knit$set(root.dir = "/Users/maxmitteldorf/Desktop/stone_data_envio")
```
![](pics/stone_logo.png)

## Introdução
Neste pequeno html buscarei fazer um resumo sobre o tratamento que conferi às bases de dados do desafio. Comentarei tanto sobre as bases de clientes e faturamento da Stone, quanto das bases adicionais que julguei interessante adicionar à análise.

## Limpeza dos dados dos clientes da Stone
O tratamento da base de dados cadastrais sobre os clientes foi meu primeiro passo. Não descreverei todas as modificações (que estão disponíveis no script "stone_data_cleaning.R"), mas buscarei tratar dos aspectos mais importantes.

Meu primeiro passo foi importar todos os pacotes que utilizaria no processo:

```{r}
library(tidyverse)
library(skimr)
library(stringi)
library(data.table)
library(lubridate)
```
\
Em seguida, importei as bases de dados referentes aos clientes e faturamentos:
```{r}
### Importing dataframes
df_cad = read_csv("data/raw/cadastrais.csv")
df_data = read_csv("data/raw/tpv-mensais-treinamento.csv")
```
\
Antes de prosseguir, juntei as bases de dados de cadastro e de faturamento:
```{r}
#### Merging dataframes
df_cad = distinct(df_cad)
df_total = inner_join(df_cad, df_data, by="id")
```
\
Em seguida, podemos investigar os tipos das variáveis e, também, dar uma olhada nas primeiras linhas da base de dados:
```{r}
#### Merging dataframes
options(dplyr.width = Inf)
head(df_total, 2)
```
\
O primeiro problema que podemos ver na base é a formatação errada das datas:
```{r}
#### Merging dataframes
head(df_total$mes_referencia, 2)
```
Resolver tal problema não é difícil:
```{r}
df_total$mes_referencia = ymd(df_total$mes_referencia)
df_total$StoneFirstTransactionDate = ymd(df_total$StoneFirstTransactionDate)
head(df_total$mes_referencia, 2)
```
\
O próximo grande problema está na coluna "Estado", que contém mais de 61 valores únicos:
```{r}
### Investigating why there are 61 unique values for "Estado"
unique(df_total$Estado) # both fullnames and initials are appearing for each state
```
Para corrigir isso utilizarei um dataframe extra com siglas e nomes de estados e, também, manipularei o padrão das strings:
```{r}
df_siglas = read_csv("data/raw/sigla_estados.csv") # Dataframe w/ state icons

## Fixing "Estado" variable
df_siglas = mutate(df_siglas, NOME = paste0(toupper(NOME),"$")) %>% # Converting to lowercase
  mutate(NOME = stri_trans_general(NOME,"Latin-ASCII"))# removing accents

## Creating named list with replacements
siglas = df_siglas$SIGLA
names(siglas) = df_siglas$NOME

df_total = mutate(df_total, Estado = toupper(Estado)) %>% # Converting to uppercase
  mutate(Estado = stri_trans_general(Estado,"Latin-ASCII")) # removing accents

## Replacing strings
df_total$Estado = str_replace_all(df_total$Estado, siglas)
unique(df_total$Estado)
```
\
O próximo problema pode ser encontrado na coluna porte, que contém em cada célula uma string com um range numérico:
```{r}
## Changing porte datatype to number
head(df_total$porte, 3)
```
Dessa forma, podemos manipular a string para tornar o range numérico e calcular sua média:
```{r}
## Changing porte datatype to number
df_total = mutate(df_total, porte=str_replace_all(porte, "2.5k", "2500"))
df_total = mutate(df_total, porte=str_replace_all(porte, "k", "000"))
df_total = mutate(df_total, porte=str_replace_all(porte, "\\+", ""))
df_total = mutate(df_total, porte=str_replace_all(porte, "-", "+"))
eval_parse = function (x) {eval(parse(text = x))}
df_total$porte = sapply(df_total$porte, eval_parse)/2
head(df_total$porte, 3)
```
\
Para encerrar a parte de limpeza de dados, vou descrever o maior problema que encontrei na base. Durante minhas manipulações, descobri que o identificador único de clientes possuía entradas repetidas para as mesmas datas. Isso ocorre porque alguns clientes tiverem algumas variáveis alteradas durante o tempo. Dessa forma, haviam clientes com mais de uma Macro Classificação, mais de um segmento, mais de uma estimativa de TPV, entre outros problemas:
```{r, eval=T, echo=T}
### The dataframe has different entries for the same id's 
distinct_values = df_total %>% 
  as_tibble() %>% 
  group_by(id) %>% 
  summarise(n_mcc = n_distinct(MCC), n_macro=n_distinct(MacroClassificacao),
            n_segmento=n_distinct(segmento), n_subsegmento=n_distinct(sub_segmento),
            n_persona=n_distinct(persona), n_porte=n_distinct(porte),
            n_tpvestimate = n_distinct(TPVEstimate), n_documento=n_distinct(tipo_documento),
            n_estado=n_distinct(Estado)) 
```
Vamos observar alguns ids com algumas dessas variáveis que se alteram:
```{r, eval=T, echo=T}
## These ids have different caracteristics
ids_errados = distinct_values %>% 
  filter(n_mcc>1 | n_macro>1 | n_segmento>1 | n_subsegmento>1 | n_persona>1 |
           n_porte>1 | n_tpvestimate>1 | n_documento>1 | n_estado>1)
head(ids_errados, 4)
```
\
Agora, podemos definir alguma estratégia para remover esses valores problemáticos. Meu critério de decisão será priorizar entradas com valor válido para estado, maior estimativa de TPV e maior porte. Julgo que esse critério selecionará as entradas mais recentes para cada id.
\
Esse é o código que utilizei para resolver isso. Não comentarei ele em detalhes pois acredito ser algo um pouco cansativo:
```{r, eval=F, echo=T}
## Defining  a strategy to keep the most recent entries for each id
linhas_certas = tibble()
contador = 1
for (id_problematico in ids_errados$id) 
{
  df_i = df_total %>% 
    as_tibble() %>% x
    filter(id==id_problematico) %>% 
    arrange(Estado, desc(TPVEstimate), desc(porte)) ## Prioritizing entries with a valid state variable
                                                    ## and with the highest TPV estimate and size
  df_i = slice_head(df_i, n=n_distinct(df_i$mes_referencia))
  
  linhas_certas = bind_rows(linhas_certas, df_i)
  print(contador)
  contador = contador + 1
}
## Eliminating wrong id's from dataframe
df_correct = df_total %>% 
  as_tibble() %>% 
  filter(!(id %in% ids_errados$id))
df_correct = bind_rows(df_correct, linhas_certas) %>% 
  arrange(id, mes_referencia)
```
O último passo é apenas eliminar os ids errados selecionados e salvar a base de dados:
```{r, eval=F, echo=T}
## Eliminating wrong id's from dataframe
df_correct = df_total %>% 
  as_tibble() %>% 
  filter(!(id %in% ids_errados$id))
df_correct = bind_rows(df_correct, linhas_certas) %>% 
  arrange(id, mes_referencia)

#### Saving cleaned dataframe
write_csv(df_correct, "Data/Clean/stone_data_cleaned.csv")
```

## Dados adicionais do ipeadata
Decidi trazer algumas variáveis (a nível de estado e de país) que acredito úteis para medir o nível de atividade econômica e de preços. São elas:

* **Nível país:**
  * Produção total de caixas de papelão no mês
  * Variação da produção de caixas de papelão no mês
  * IPCA mensal
  * Taxa de câmbio mensal
* **Nível estado:**
  * Número de demissões no mês (dados do CAGED)
  * Variação da produção de caixas de papelão 
  * Índice do varejo no estado

Todas essas variáveis estavam bem formatadas, motivo pelo qual decidi omitir o código que une tais variáveis com as da Stone.

## Análise Exploratória

A partir de agora, buscarei conduzir uma análise explorátória da base de dados unificada, com dados da Stone e do ipeadata. Nesta etapa, focarei meus comentários na análise dos resultados e nos trechos de código mais importantes. Omitirei os códigos para os gráficos porque eles ocupariam um espaço muito grande.

### `TPV Mensal`

Primeiramente, podemos carregar as bases e, também, a fonte que decidi usar nos gráficos:
```{r, eval=T, echo=F}
library(tidyverse)
library(showtext)
### Importing dataframe and fonts
df = read_csv("data/clean/spine_stone_ipea.csv")
font_add(family = "Sharon", regular = "visualization/fonts/Sharon.ttf")
showtext_auto()
```
Começarei pelo TPV Mensal, nossa variável explicativa e, portanto, a mais importante. Vamos observar sua distribuição:
```{r, eval=T, echo=F}
### Monthly TPV histogram
ggplot(df, aes(x=TPV_mensal)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white", bins=80)+
  geom_density(alpha=.7, fill="#00a868") +
  xlim(0, 100000) +
  theme_bw() + ylab("Densidade")+
  theme(axis.text.x = element_text(colour="black", size=8), axis.text.y = element_text(colour="black", size=8))+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  theme(panel.grid.major = element_line(size=0, color="black"))+
  theme(panel.grid.minor = element_line(size=0, color="black"))+
  xlab("TPV Mensal") + ggtitle("Distribuição do TPV Mensal")
```
\
É possível observar que a distribuição dos TPVs mensais possui uma grande assimetria positiva, indicando que a maior parte das empresas e clientes Stone concentram-se em faixas menores de faturamento.
\
\
Vamos investigar mais um pouco a distribuição do TPV, ver como ela se distribui pelos estados e, agrupar a base por estado para calcular a média do TPV de cada um:
```{r, eval=T, echo=T}
### Number of clients by state
df %>% group_by(Estado) %>% 
  summarise(TPV_mensal = mean(TPV_mensal)) %>% 
  head(n=10)
```
Agora já podemos desenhar o gráfico de barras:
```{r, eval=T, echo=F}
## Agrupando
df_est = df %>% group_by(Estado) %>% 
  summarise(TPV_mensal = mean(TPV_mensal)) %>% 
  drop_na() %>% 
  arrange(TPV_mensal)

## Ordem
limits_est = df_est$Estado

## Gráfico
ggplot(df_est, mapping=aes(x=Estado, y=TPV_mensal, fill=TPV_mensal)) +
  geom_col() +
  theme_bw() +
  ylab("Média do TPV") +
  theme(legend.position = "none")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Média do TPV Mensal por Estado da Federação")+
    theme(axis.text.x = element_text(colour="black", size=8), axis.text.y = element_text(colour="black", size=10))+
    theme(panel.grid.major = element_line(size=0, color="black"))+
  theme(panel.grid.minor = element_line(size=0, color="black"))+
  scale_x_discrete(limits = limits_est) + scale_fill_gradient(low = "#b7e4c7", high="#1b4332")
```
\
Podemos ver que Roraima é o estado com a maior média de TPV mensal. Isso pode parecer surpreendente, a princípio, mas devemos nos atentar também à quantidade de clientes nesse estado para verificar se isso não é efeito de algum outlier. Do outro lado, o estado com menor faturamento médio é o Acre. Apesar desses valores não esperados, vemos uma tendência geral dos estados mais ricos terem uma média de faturamento maior.
\
\
Agora, vamos investigar o número de clientes por estado para confirmar a suspeita sobre Roraima:
```{r, echo=T, eval=T}
### Number of clients by state
states = df %>% 
  as_tibble() %>% 
  count(Estado) %>% 
  drop_na() %>% 
  rename("n_clientes"=n) %>%
  arrange(n_clientes)

head(states, n=5)
```
```{r, echo=F, eval=T}
order_nst = states$Estado
ggplot(states, mapping=aes(x=Estado, y=n_clientes, fill=n_clientes)) +
  geom_col() +
  theme_bw() +
  ylab("Número de clientes") +
  theme(legend.position = "none") +
  scale_x_discrete(limits=order_nst)+
  ggtitle("Clientes Stone por Estado")+
      theme(axis.text.x = element_text(colour="black", size=8), axis.text.y = element_text(colour="black", size=9))+
      theme(panel.grid.major = element_line(size=0, color="black"))+
  theme(panel.grid.minor = element_line(size=0, color="black"))+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  scale_fill_gradient(low = "#40916c", high="#1b4332")
```
\
Podemos ver que nossa suspeita sobre Roraima se confirmou. Muito provavelmente existe algum outlier com faturamento muito alto em Roraima. Além disso, vemos que São Paulo tem mais que o dobro de clientes que o segundo colocado (Rio de Janeiro). Vemos que a Stone tem uma presença maior na região Sudeste, seguida pela região Sul.
\
\
Agora, vamos investigar como varia a média do TPV para cada Macro Classificação de setores:
\
Antes de fazer o gráfico, precisamos agrupar o dataframe por Macro Classificação: 
```{r, eval=T, echo=T}
## Agrupando
df_est = df %>% group_by(MacroClassificacao) %>% 
  summarise(TPV_mensal = mean(TPV_mensal)) %>% 
  drop_na() %>% 
  arrange(TPV_mensal)

## Ordem
limits_mac = df_est$MacroClassificacao
```
```{r, eval=T, echo=F}
## Gráfico
ggplot(df_est, mapping=aes(x=MacroClassificacao, y=TPV_mensal, fill=TPV_mensal)) +
  geom_col() +
  theme_bw() +
  ylab("Média do TPV") +
  xlab("Macro Classificação")+
  theme(legend.position = "none")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Média do TPV Mensal por Macro Classificação")+
    theme(axis.text.x = element_text(colour="black", size=10), axis.text.y = element_text(colour="black", size=10))+
    theme(panel.grid.major = element_line(size=0, color="black"))+
  theme(panel.grid.minor = element_line(size=0, color="black"))+
    scale_x_discrete(limits = limits_mac) + scale_fill_gradient(low = "#b7e4c7", high="#1b4332")+
  coord_flip()
```
\
Podemos ver que, dentre as áreas de Macro Classificação, os postos de gasolina apresentam um faturameto muito superior em relação aos demais grupos.
\
\
Vamos investigar um pouco mais a fundo a média do TPV entre diversos setores. Dessa vez vamos agrupar a base pela variável segmento:
```{r, eval=T, echo=T}
## Agrupando
df_seg = df %>% group_by(segmento) %>% 
  summarise(TPV_mensal = mean(TPV_mensal)) %>% 
  drop_na() %>% 
  arrange(TPV_mensal)

## Ordem
limits_seg = df_seg$segmento
```
```{r, eval=T, echo=F}
## Gráfico
ggplot(df_seg, mapping=aes(x=segmento, y=TPV_mensal, fill=TPV_mensal)) +
  geom_col() +
  theme_bw() +
  ylab("Média do TPV") +
  xlab("Segmento")+
  theme(legend.position = "none")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Média do TPV Mensal por Segmento")+
    theme(axis.text.x = element_text(colour="black", size=10), axis.text.y = element_text(colour="black", size=9))+
    theme(panel.grid.major = element_line(size=0, color="black"))+
  theme(panel.grid.minor = element_line(size=0, color="black"))+
  scale_x_discrete(limits = limits_seg) + scale_fill_gradient(low = "#b7e4c7", high="#1b4332")+
  coord_flip()
```
\
Podemos ver que os segmentos de logística e de postos de gasolina são os com maior média de faturamento. Serviços de beleza e companhias aparecem por último. Apesar disso, o valor de companhias aéreas parece ser anormalmente baixo.

### `Variáveis do ipeadata`

Agora, vamos analisar um pouco as variáveis do ipeadata. A melhor forma de começar é construindo um heatmap das correlações entre elas e o TPV mensal:

```{r, eval=T, echo=F}
library(reshape2)

# Creating matrix
df_heatmap = df %>% 
  select(TPV_mensal, caixas_papelao, caixas_papelao_varpct, ipca_pct, cambio_dol, demissoes, ind_varejo) %>% 
  drop_na()
mat_heatmap = round(cor(df_heatmap), 2)
melted_mat = melt(mat_heatmap)
## Gráfico
ggplot(data = melted_mat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "red", high = "#00a868", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlação") +
  theme_bw()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1, color="black"), axis.text.y = element_text(color = "black"),
    axis.title=element_blank())+
 coord_fixed()+
    theme(text=element_text(family="Sharon",size=12, color = "black")) +
  ggtitle("Matriz de Correlação")
```
\
Observando-se a matriz, vemos que as correlações entre as variáveis do ipeadata e o TPV mensal não são muito altas. Apesar disso, vemos uma correlação ligeiramente positiva com o ipca e o índice de varejo, e uma correlação lugeiramente negativa com o dólar. Esses efeitos estão dentro do esperado, uma vez que o nível de câmbio e da inflação tem um efeito direto no faturamento. Devemos nos manter atentos porque há diversos segmentos de clientes que podem ter diferentes interações com essas variáveis, que acabam mascaradas quando agregadas.
\
\
Ainda, quero ressaltar que será adicionado lag temporal e médias móveis a todas essas variáveis posteriormente. Uma vez que esperamos uma forte característica autoregressiva no modelo.
\
\
Como acabei de comentar, vamos ver o impacto dessas variáveis no TPV de forma mais detalhada.
Primeiramente, vamos começar vendo o impacto do nível do ipca de acordo com as Macro Classificações de clientes:
```{r, echo=F, eval=T}
tpv_medio = df %>% 
  drop_na %>% 
  group_by(mes_referencia, MacroClassificacao) %>% 
  summarise(med_tpv=mean(TPV_mensal), caixas_papelao_varpct=mean(caixas_papelao_varpct),
            caixas_papelao=mean(caixas_papelao), ipca = mean(ipca_pct), cambio_dol = mean(cambio_dol), demissoes=mean(demissoes), ind_varejo = mean(ind_varejo))

ggplot(data = tpv_medio, mapping=aes(x=ipca, y=med_tpv)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fill="#00a868", color="#00a868") +
  facet_wrap(~MacroClassificacao) +
  theme_bw()+ ylab("TPV Médio") + xlab("IPCA")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Relação entre TPV e IPCA por Macro Classificação")+
      theme(axis.text.x = element_text(colour="black", size=10), axis.text.y = element_text(colour="black", size=10))
```
\
Observando os gráficos, podemos ver uma ligeira tendência positiva (embora pequena) entre inflação(IPCA) e o TPV. Além disso, vemos que o faturamento de postos de gasolina é o mais sensível à inflação. Provavelmente, por ser um setor monopolizado, o ajuste dos preços ocorre mais rapidamente.
\
\
Vamos, agora, construir o mesmo gráfico para a taxa de câmbio e ver se nossas expectativas estavam corretas:
```{r, echo=F, eval=T}
ggplot(data = tpv_medio, mapping=aes(x=cambio_dol, y=med_tpv)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fill="#00a868", color="#00a868") +
  facet_wrap(~MacroClassificacao) +
  theme_bw()+ ylab("TPV Médio") + xlab("USD em R$")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Relação entre TPV e Dólar por Macro Classificação")+
      theme(axis.text.x = element_text(colour="black", size=10), axis.text.y = element_text(colour="black", size=10))
```
\
Estávamos corretos! É possível ver uma clara relação negativa entre o preço do dólar em reais e o TPV. O setor de postos tem seu faturamento muito prejudicado com um dólar mais apreciado. Por outro lado, vemos que o setor de supermercados e farmácias tem um aumento em seu faturamento (provavelmente por serem setores essenciais). Varejo e entretenimento também são prejudicados pelo aumento do dólar.
\
\
O que aprendemos com isso? Que analisar a correlação, com todos os setores agrupados, esconde o poder preditivo de muitas variáveis. Dessa forma, nosso "feeling econômico" de que essas variáveis eram importantes estava correto.
\
\
Vamos observar, agora, a relação entre o TPV e o índice de nível do varejo. É de se esperar que tal relação seja mais forte para o macro setor de varejo. Vamos ver:
\
```{r, echo=F, eval=T}
ggplot(data = tpv_medio, mapping=aes(x=ind_varejo, y=med_tpv)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fill="#00a868", color="#00a868") +
  facet_wrap(~MacroClassificacao) +
  theme_bw()+ ylab("TPV Médio") + xlab("Índice de Varejo")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Relação entre TPV e Índ. de Varejo por Macro Classificação")+
      theme(axis.text.x = element_text(colour="black", size=10), axis.text.y = element_text(colour="black", size=10))
```
\
Mais uma vez, vemos que nossa intuição econômica estava correta. O índice de varejo aparenta ser uma boa proxy para a performance econômica deste setor e de outros e, certamente, contribuirá com o poder preditivo do modelo.
\
\
Por último, vamos visualizar a relação entre o número de demissões contabilizado pelo CAGED e o TPV Mensal:
```{r, echo=F, eval=T}
ggplot(data = tpv_medio, mapping=aes(x=demissoes, y=med_tpv)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fill="#00a868", color="#00a868") +
  facet_wrap(~MacroClassificacao) +
  theme_bw()+ ylab("TPV Médio") + xlab("Demissões")+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  ggtitle("Relação entre TPV e Demissões por Macro Classificação")+
      theme(axis.text.x = element_text(colour="black", size=4), axis.text.y = element_text(colour="black", size=10))
```
\
Neste caso, a relação não fica tão clara ou interpretável. Vemos que o faturamento de postos de gasolina claramente aumenta em meses com mais demissões, algo muito interessante.
\
\
Para finalizar, gostaria de dizer, mais uma vez, que todas as variáveis que trouxe do ipea podem ter uma importância especial para um grupo específico de clientes. Dessa forma, é válido deixá-las no modelo. Não adicionei mais variáveis pois pretendo adicionar vários lags temporais e médias móveis para todas as variáveis listadas. Dessa forma, espero que o número de variáveis regressoras aumente de forma substancial na etapa de feature engineering.

## Feature Engineering
Minha estratégia de feature engineering foi bastante convencional. Meu método foi aplicar uma série de lags e médias móveis à todas as variáveis, a fim de evitar data leakage entre os períodos.
\
Vale comentar que a partir desse momento continuei o desafio com Python.
\
Primeiramente, podemos importar os pacotes e os dataframes utilizados no script. Nesse markdown utilizarei apenas as primeiras 1000 linhas do dataframe por ser somente uma demonstração dos métodos aplicados:
```{python, eval=T}
from stone_data_challenge.config import data_dir
from sklearn.impute import SimpleImputer
import datetime as dt
import numpy as np
import datetime
import pandas as pd

# # Reading dataframe
pd.set_option('display.max_columns', 10)
pd.set_option('display.width', 1000)
df = pd.read_csv(f"{data_dir}/data/clean/spine_stone_ipea.csv")
df = df.head(100000)
df = df.replace("nan", np.NaN)
df = df.drop_duplicates()
df.head(5)
```
\
Primeiramente, vamos checar os tipos das variáveis para ver se podemos armazená-las em algum tipo de dado mais eficiente:
```{python, eval=T}
df.dtypes
```
\
Vamos agora corrigir os tipos de algumas variáveis:
```{python, eval=T}
# # Fixing types
df[["MCC", "MacroClassificacao", "StoneCreatedDate", "StoneFirstTransactionDate", "segmento",
    "sub_segmento", "persona", "tipo_documento", "Estado", "mes_referencia", "id"]] = df[["MCC", "MacroClassificacao", "StoneCreatedDate", "StoneFirstTransactionDate", "segmento", "sub_segmento", "persona", "tipo_documento", "Estado", "mes_referencia", "id"]].astype(str)
df.dtypes
```
\
Agora podemos consertar o formato das datas:
```{python, eval=T}
# # Fixing date formatting
df["StoneCreatedDate"] = pd.to_datetime(df["StoneCreatedDate"]).dt.tz_localize(None)
df["StoneFirstTransactionDate"] = pd.to_datetime(df["StoneFirstTransactionDate"])
df["mes_referencia"] = pd.to_datetime(df["mes_referencia"])
df[["StoneFirstTransactionDate", "StoneCreatedDate"]].head(5)
```
\
Algo que podemos fazer para o modelo é transformar as variáveis sobre a data de cadastro do cliente e sua primeira transação em números ordinais. Observe:
```{python, eval=T}
# # Transforming date variables in cardinal numbers
initial_date = dt.datetime(2020, 7, 31)
df["DaysSinceCreation"] = -(df["StoneCreatedDate"] - initial_date).dt.days
df["DaysSinceFirstTrans"] = -(df["StoneFirstTransactionDate"] - initial_date).dt.days
df[["DaysSinceCreation", "DaysSinceFirstTrans"]]
```

\
Para que nosso modelo possa identificar os efeitos de sazonalidade, é interessante dividir a coluna "mes_referencia" em duas variáveis diferentes: uma para mês e outra para ano. Apliquei o mesmo tratamento para "StoneCreatedDate" e "StoneFirstTransactionDate".
```{python, eval=T}
# # Creating separate year and month variables for date imputing
df["YearCreated"] = df["StoneCreatedDate"].dt.year
df["MonthCreated"] = df["StoneCreatedDate"].dt.month
df["YearFirstTrans"] = df["StoneFirstTransactionDate"].dt.year
df["MonthFirstTrans"] = df["StoneFirstTransactionDate"].dt.month
df["AnoReferencia"] = df["mes_referencia"].dt.year
df["MesReferencia"] = df["mes_referencia"].dt.month

df[["YearCreated", "MonthCreated", "YearFirstTrans", "MonthFirstTrans", "AnoReferencia", "MesReferencia"]]
```
\
Criadas essas variáveis, agora podemos remover as variáveis de data não ordinais, com excessão de "mes_referencia" que será utilizada como índice mais adiante:
```{python, eval=T}
# # # Removing unwanted variables
df.drop(["mes", "StoneFirstTransactionDate", "StoneCreatedDate"], axis="columns", inplace=True)
```
\
Vamos checar a porcentagem de valores faltantes em cada coluna no dataframe:
```{python, eval=T}
# # Checking for missing data
df.replace('nan', np.nan, inplace=True)
percent_missing = df.isna().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({"col_name": df.columns, "pct_missing": percent_missing})
missing_value_df
```
\
Primeiramente, podemos imputar alguns valores categóricos como "Desconhecido" para as colunas referentes ao setor dos clientes:
```{python, eval=T}
# # Checking for missing data
df["MacroClassificacao"] = df["MacroClassificacao"].fillna("Desconhecido")
df["segmento"] = df["segmento"].fillna("Desconhecido")
df["sub_segmento"] = df["sub_segmento"].fillna("Desconhecido")
```
\
Da análise exploratória, sabemos que São Paulo é, de longe, o estado com maior número de clientes Stone. Dessa forma podemos imputar os valores faltantes com São Paulo:
```{python, eval=T}
# # Imputing missing categorical data for state
df["Estado"] = df["Estado"].fillna("SP")
```
\
Por último, sobraram os valores faltantes para as colunas "ind_varejo", "demissoes", "DaysSinceFirstTrans", "YearFirstTrans"e "MonthFirstTrans".
Para imputar os valores nulos, utilizarei o imputador do scikit-learn e substituirei os valores faltantes pela mediana:
```{python, eval=T}
# Creating column transformer
missing_vars = ["ind_varejo", "demissoes", "DaysSinceFirstTrans", "YearFirstTrans", "MonthFirstTrans"]
imputer = SimpleImputer(strategy="median")
df[missing_vars] = imputer.fit_transform(df[missing_vars])  # Imputing data
```
\
Agora, podemos conferir que não há mais valores nulos:
```{python, eval=T}
# # Checking for missing data
percent_missing = df.isnull().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({"col_name": df.columns, "pct_missing": percent_missing})
missing_value_df
```
Chegamos em um momento muito importante. Pela forte característica autoregressiva de nosso problema, devemos gerar uma série de variáveis com defasagem temporal de 1 e mais períodos. Também é interessante gerar médias móveis para distintas janelas de tempo.
\
\
Antes de começar a adicionar variáveis, devemos adicionar 5 meses à base de dados, para que possamos utilizar essas variáveis com lag e médias móveis na previsão final do desafio. Esse é o código que utilizei para isso:
```{python, eval=T}
df = df.set_index(["mes_referencia"])

# # We need to add 5 months to the dataset before shifting the columns
df_agosto = df.groupby("id").tail(1)
df_agosto.index = df_agosto.index + datetime.timedelta(days=30)
df_agosto.loc[:, "TPV_mensal"] = np.nan
df_agosto.loc[:, "caixas_papelao"] = np.nan
df_agosto.loc[:, "caixas_papelao_varpct"] = np.nan
df_agosto.loc[:, "ipca_pct"] = np.nan
df_agosto.loc[:, "cambio_dol"] = np.nan
df_agosto.loc[:, "demissoes"] = np.nan
df_agosto.loc[:, "ind_varejo"] = np.nan
df_agosto[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_agosto[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_agosto["MesReferencia"] = df_agosto["MesReferencia"] + 1

df_setembro = df_agosto[:]
df_setembro.index = df_setembro.index + datetime.timedelta(days=30)
df_setembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_setembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_setembro["MesReferencia"] = df_setembro["MesReferencia"] + 1

df_outubro = df_setembro[:]
df_outubro.index = df_outubro.index + datetime.timedelta(days=30)
df_outubro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_outubro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_outubro["MesReferencia"] = df_outubro["MesReferencia"] + 1

df_novembro = df_outubro[:]
df_novembro.index = df_novembro.index + datetime.timedelta(days=30)
df_novembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_novembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_novembro["MesReferencia"] = df_novembro["MesReferencia"] + 1

df_dezembro = df_novembro[:]
df_dezembro.index = df_dezembro.index + datetime.timedelta(days=30)
df_dezembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] = df_dezembro[["DaysSinceCreation", "DaysSinceFirstTrans"]] + 30
df_dezembro["MesReferencia"] = df_dezembro["MesReferencia"] + 1

df = pd.concat([df, df_agosto,df_setembro,df_outubro,df_novembro,df_dezembro], axis=0)
df = df.sort_values(["id", "mes_referencia"])
```
\
Agora, podemos conferir que temos mais 5 meses na base de dados:
```{python, eval=T, echo=T}
df.tail(8)
```
\
\
Vamos começar a adicionar as médias móveis e variáveis com lag. Vou exemplificar o tratamento que conferi ao TPV mensal. Repeti exatamente o mesmo procedimento para as outras variáveis:
```{python, eval=T, echo=T}
# Lag TPV
df["TPV_lag_1"] = df.groupby("id")["TPV_mensal"].shift(1)
df["TPV_lag_2"] = df.groupby("id")["TPV_mensal"].shift(2)
df["TPV_lag_3"] = df.groupby("id")["TPV_mensal"].shift(3)
df["TPV_lag_4"] = df.groupby("id")["TPV_mensal"].shift(4)
df["TPV_lag_5"] = df.groupby("id")["TPV_mensal"].shift(5)
df["TPV_lag_6"] = df.groupby("id")["TPV_mensal"].shift(6)
df["TPV_lag_12"] = df.groupby("id")["TPV_mensal"].shift(12)

# Creating moving averages for TPV
df["TPV_MA_2"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(2, 2).mean())
df["TPV_MA_3"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(3, 2).mean())
df["TPV_MA_4"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(4, 2).mean())
df["TPV_MA_5"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(5, 2).mean())
df["TPV_MA_6"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(6, 2).mean())
df["TPV_MA_12"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(12, 2).mean())
df["TPV_MA_18"] = df.groupby("id")["TPV_mensal"].transform(lambda x: x.rolling(18, 2).mean())
```
\
Vamos verificar que essas transformações funcionaram:
```{python, eval=T, echo=T}
df[["TPV_mensal","TPV_lag_1","TPV_lag_3","TPV_lag_3","TPV_lag_4","TPV_MA_2","TPV_MA_3","TPV_MA_4"]].tail(10)
```
\
Por último, após aplicar tal procedimento em todas as variáveis, podemos salvar o dataframe final para a modelagem:
```{python, eval=F, echo=T}
# Writing dataframe
df = df.reset_index()
df.to_csv(f"{data_dir}/feat_eng/model_data_v5.csv", index=False)
```

## Modelagem
Diante de um problema com um componente temporal, e que exige a previsão de 5 lags para frente, optei por construir 5 modelos independentes (um para cada lag temporal).
\
\
Decidi por utilizar o [LightGBM](https://lightgbm.readthedocs.io/en/latest/) para fazer as previsões por sua rapidez e robustez. Diante de um grande número de hiperparâmetros, também optei por utilizar o pacote [Optuna](https://optuna.org/) para otimizá-los.
\
\
Mostrarei o código para o modelo que prevê um lag temporal (t+1). O código para os modelos (t+2) em diante são muito similares e estão disponíveis no repositório.
\
\
Primeiramente, vamos importar os pacotes necessários:
```{python, eval=F, echo=T}
import pandas as pd
import numpy as np
from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples
from sklearn.utils.validation import _deprecate_positional_args
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import optuna
import pickle
```
\
Em seguida, as configurações do script devem ser definidas:
* `TUNING` - Indica se a otimização dos parâmetros será realizada
* `TUNING_SIZE` - Indica o tamanho do dataframe para a otimização dos parâmetros
* `TRAINING` - Indica se o modelo será treinado ou não
* `TRAINING_SIZE` - Indica o tamanho do dataframe para o treino do modelo
* `FINNAL_PREDICTION` - Indica se queremos treinar o modelo em todos os dados e exportar a previsão final do desafio
\
\
Agora, podemos começar lendo o dataframe e removendo as colunas sem lag temporal para evitarmos problemas de data leakage:
```{python, eval=F, echo=T}
# # Reading dataframe
print("Reading Dataframe")
df = pd.read_csv(f"{data_dir}/data/feat_eng/model_data_v5.csv")
print("Dataframe succesfully readed")

# # Dropping columns without lag e MA2
df.drop(["caixas_papelao", "caixas_papelao_varpct", "ipca_pct", "cambio_dol", "demissoes", "ind_varejo",
         "TPV_MA_2", "ipca_MA_2", "caixas_papelao_MA_2", "caixas_papelao_varpct_MA_2",
         "demissoes_MA_2", "ind_varejo_MA_2", "cambio_dol_MA_2"], axis="columns", inplace=True)


# # Renaming TPV columns
df.rename(columns={"TPV_mensal": "TPV_t1"}, inplace=True)
```
\
Em seguida, usei o LabelEncoder() para transformar as variáveis discretas em numéricas. Utilizei o LabelEncoder() no lugar do OneHotEcoder() porque modelos de árvore como o LightGBM lidam bem com tal formato de dados.
```{python, eval=F, echo=T}
# # Using LabelEncoder for categorical variables
# # Lightgbm can handle label encoded variables very efficiently
le = LabelEncoder()
df["MCC"] = le.fit_transform(df["MCC"])
df["MacroClassificacao"] = le.fit_transform(df["MacroClassificacao"])
df["segmento"] = le.fit_transform(df["segmento"])
df["sub_segmento"] = le.fit_transform(df["sub_segmento"])
df["persona"] = le.fit_transform(df["persona"])
df["Estado"] = le.fit_transform(df["Estado"])
df["tipo_documento"] = le.fit_transform(df["tipo_documento"])
```
\
Sendo assim, também podemos separar os meses da previsão final (agosto até dezembro) da base de dados e separá-los para um momento posterior:
```{python, eval=F, echo=T}
# # Saving dataset for prediction
df["mes_referencia"] = pd.to_datetime(df["mes_referencia"])
df_prediction = df.groupby("id").tail(5).set_index("mes_referencia")
df_prediction_august = df_prediction.groupby("id").head(1).drop("TPV_t1", axis="columns")
df_prediction_august["ind"] = df_prediction_august["id"]
df_prediction_august = df_prediction_august.set_index("ind")
df = df[df["mes_referencia"] < "2020-08-30"]
```
\
Agora, como desejamos poder avaliar as métricas de performance do modelo, devemos remover os próximos 5 meses de dados para deixá-los como base de validação final. Também tive que fazer algumas manipulações para remover os dados de algumas variáveis regressoras que não deveriam estar disponíveis no momento da execução do modelo.
```{python, eval=F, echo=T}
df_val = df.groupby("id").tail(5).groupby("id").head(1)
df_val = df_val[pd.notnull(df_val["TPV_t1"])].set_index("mes_referencia")

df_train = df.drop(df.groupby("id").tail(5).index, axis=0).set_index("mes_referencia")
df_train = df_train[pd.notnull(df_train["TPV_t1"])]
df_train = df_train.head(TRAINING_SIZE)

df = df.set_index("mes_referencia")
df = df[pd.notnull(df["TPV_t1"])]

# # Saving column names for later
colnames = df.columns

# # Saving categorical vars names
x_colnames = list(df.drop("TPV_t1", axis="columns").columns.values)
categorical_vars = ["id", "MCC", "MacroClassificacao", "segmento", "sub_segmento", "persona", "tipo_documento", "Estado"]
```
\
Agora, gostaria de falar um pouco sobre minha estratégia para separar as bases de treino e teste. Estamos diante de um problema de séries temporais, por isso, a princípio, poderíamos utilizar o TimeSerisSplit() para treinar nosso modelo em vários ranges de tempo sem que ocorra time leakage. A questão é que nosso problema é de séries temporais com grupos (nesse caso o id dos clientes). Dessa forma, o TimeSeriesSplit() poderia misturar clientes com ids diferentes, o que não queremos de jeito alguma.
\
\
É aí que entra o GroupTimeSeriesSplit(), uma estratégia de divisão que respeita tanto a temporalidade, quanto o grupo das observações. Essa funcionalidade ainda não está completamente implementada no scikit-learn, mas está em vias de ser. Dessa forma, conseguimos acessar o código para ela nesse [link](https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243).
```{python, eval=F, echo=T}
class GroupTimeSeriesSplit(_BaseKFold):
    """Time Series cross-validator variant with non-overlapping groups.
    Provides train/test indices to split time series data samples
    that are observed at fixed time intervals according to a
    third-party provided group.
    In each split, test indices must be higher than before, and thus shuffling
    in cross validator is inappropriate.
    This cross-validation object is a variation of :class:`KFold`.
    In the kth split, it returns first k folds as train set and the
    (k+1)th fold as test set.
    The same group will not appear in two different folds (the number of
    distinct groups has to be at least equal to the number of folds).
    Note that unlike standard cross-validation methods, successive
    training sets are supersets of those that come before them.
    Read more in the :ref:`User Guide <cross_validation>`.
    Parameters
    ----------
    n_splits : int, default=5
        Number of splits. Must be at least 2.
    max_train_size : int, default=None
        Maximum size for a single training set.
    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.model_selection import GroupTimeSeriesSplit
    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\
                           'b', 'b', 'b', 'b', 'b',\
                           'c', 'c', 'c', 'c',\
                           'd', 'd', 'd'])
    >>> gtss = GroupTimeSeriesSplit(n_splits=3)
    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):
    ...     print("TRAIN:", train_idx, "TEST:", test_idx)
    ...     print("TRAIN GROUP:", groups[train_idx],\
                  "TEST GROUP:", groups[test_idx])
    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\
    TEST GROUP: ['b' 'b' 'b' 'b' 'b']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\
    TEST GROUP: ['c' 'c' 'c' 'c']
    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\
    TEST: [15, 16, 17]
    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\
    TEST GROUP: ['d' 'd' 'd']
    """

    @_deprecate_positional_args
    def __init__(self,
                 n_splits=5,
                 *,
                 max_train_size=None
                 ):
        super().__init__(n_splits, shuffle=False, random_state=None)
        self.max_train_size = max_train_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set.
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data, where n_samples is the number of samples
            and n_features is the number of features.
        y : array-like of shape (n_samples,)
            Always ignored, exists for compatibility.
        groups : array-like of shape (n_samples,)
            Group labels for the samples used while splitting the dataset into
            train/test set.
        Yields
        ------
        train : ndarray
            The training set indices for that split.
        test : ndarray
            The testing set indices for that split.
        """
        if groups is None:
            raise ValueError(
                "The 'groups' parameter should not be None")
        X, y, groups = indexable(X, y, groups)
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        group_dict = {}
        u, ind = np.unique(groups, return_index=True)
        unique_groups = u[np.argsort(ind)]
        n_samples = _num_samples(X)
        n_groups = _num_samples(unique_groups)
        for idx in np.arange(n_samples):
            if (groups[idx] in group_dict):
                group_dict[groups[idx]].append(idx)
            else:
                group_dict[groups[idx]] = [idx]
        if n_folds > n_groups:
            raise ValueError(
                ("Cannot have number of folds={0} greater than"
                 " the number of groups={1}").format(n_folds,
                                                     n_groups))
        group_test_size = n_groups // n_folds
        group_test_starts = range(n_groups - n_splits * group_test_size,
                                  n_groups, group_test_size)
        for group_test_start in group_test_starts:
            train_array = []
            test_array = []
            for train_group_idx in unique_groups[:group_test_start]:
                train_array_tmp = group_dict[train_group_idx]
                train_array = np.sort(np.unique(
                    np.concatenate((train_array,
                                    train_array_tmp)),
                    axis=None), axis=None)
            train_end = train_array.size
            if self.max_train_size and self.max_train_size < train_end:
                train_array = train_array[train_end -
                                          self.max_train_size:train_end]
            for test_group_idx in unique_groups[group_test_start:
            group_test_start +
            group_test_size]:
                test_array_tmp = group_dict[test_group_idx]
                test_array = np.sort(np.unique(
                    np.concatenate((test_array,
                                    test_array_tmp)),
                    axis=None), axis=None)
            yield [int(i) for i in train_array], [int(i) for i in test_array]
```
\
A partir daqui, podemos definir uma função de otimização para o Optuna e realizar o tuning dos parâmetros do LightGBM em uma porção do dataframe. Após a otimização, os melhores parâmetros são salvados em um dicionário através de um pickle.
```{python, eval=F, echo=T}
if TUNING:
    df_train_tuning = df_train.head(TUNING_SIZE)

    # # Objective Function
    def objective(trial, cv_fold_func=np.average):
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'boosting': 'rf',
            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
            'num_leaves': trial.suggest_int('num_leaves', 2, 200),
            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10)}

        # fitting and returning RMSE scores
        mae_list = []
        for train_idx, test_idx in GroupTimeSeriesSplit().split(df_train_tuning, groups=df_train_tuning["id"]):
            train_x, test_x = df_train_tuning.drop("TPV_t1", axis="columns").values[train_idx], df_train_tuning.drop("TPV_t1", axis="columns").values[test_idx]
            train_y, test_y = df_train_tuning["TPV_t1"][train_idx], df_train_tuning["TPV_t1"][test_idx]

            train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)

            model = lgb.train(params, train_data)
            pred = model.predict(test_x)
            mae = mean_absolute_error(pred, test_y)
            mae_list.append(mae)

        print("Trial done: MAE values on folds: {}".format(mae_list))
        return cv_fold_func(mae_list)


    # # Optuna results
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=80)
    print("Number of finished trials:", len(study.trials))
    print("Best trial:", study.best_trial.params)
    best_params = study.best_trial.params

    # Building parameter dictionary
    fixed_params = {'objective': 'regression',
                    'metric': 'mae',
                    'boosting': 'gbdt',
                    'num_threads': 4}

    best_params = {**best_params, **fixed_params}

    # # Saving best params
    filename = f"{data_dir}/data/model/parameters/best_params_t1.pickle"
    outfile = open(filename, "wb")
    pickle.dump(best_params, outfile)
    outfile.close()

else:
    # # Reading optimal parameters
    infile = open(f"{data_dir}/data/model/parameters/best_params_t1.pickle", "rb")
    best_params = pickle.load(infile)
```
\
Agora, com uma boa seleção de parâmetros, podemos partir para o treino do LightGBM com o objetivo de avaliar a performance nos dados que deixamos separados. Depois disso, salvamos em pickles o erro absoluto médio do modelo, a importância das variáveis, e as previsões na base de avaliação:
```{python, eval=F, echo=T}
if TRAINING:
    # # Training model
    print("Splitting train and test sets...")
    for train_idx, test_idx in GroupTimeSeriesSplit().split(df_train, groups=df_train["id"]):
        train_x, test_x = df_train.drop("TPV_t1", axis="columns").values[train_idx], df_train.drop("TPV_t1", axis="columns").values[test_idx]
        train_y, test_y = df_train["TPV_t1"][train_idx], df_train["TPV_t1"][test_idx]
    print("Beginning training...")
    train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    test_data = lgb.Dataset(test_x, label=test_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    lgbm_model = lgb.train(best_params, train_data, 250, valid_sets=test_data, early_stopping_rounds=50, verbose_eval=10)

    # # Predicting on Test Data
    # Creating validation sets for each month
    df_val_t1_y = df_val["TPV_t1"]
    df_val_t1_x = df_val.drop(["TPV_t1"], axis="columns")
    y_pred_lgb_test = lgbm_model.predict(df_val_t1_x, num_iteration=lgbm_model.best_iteration)

    # # Model performance on test data
    MAE_lgb_test = mean_absolute_error(y_pred_lgb_test, df_val_t1_y)

    # # Feature importances
    imp = pd.DataFrame(lgbm_model.feature_importance()).T
    imp.columns = x_colnames
    imp = imp.T
    imp.columns = ["feat_importance"]
    imp = imp.sort_values("feat_importance")

    # Saving Results
    imp.to_csv(f"{data_dir}/data/model/performance/importances_t1.csv")

    # Saving Results
    filename = f"{data_dir}/data/model/performance/MAE_t1.pickle"
    outfile = open(filename, "wb")
    pickle.dump(MAE_lgb_test, outfile)
    outfile.close()

    # # Saving test prediction

    # Joining test set and predictions
    df_y_pred = pd.DataFrame(y_pred_lgb_test, columns=["TPV_pred_t1"])
    df_val_t1_x = df_val_t1_x.reset_index()
    df_val_t1_y = pd.DataFrame(df_val_t1_y)
    df_val_t1_y = df_val_t1_y.reset_index().drop("mes_referencia", axis="columns")
    df_pred_t1 = pd.concat([df_val_t1_x, df_val_t1_y, df_y_pred], axis="columns").set_index("mes_referencia")
    df_pred_t1 = df_pred_t1[["id", "TPV_t1", "TPV_pred_t1"]]

    # Saving dataframe
    df_pred_t1.to_csv(f"{data_dir}/data/performance/test_pred_t1.csv")
```
\
Por último após termos decidido que estamos satisfeitos com o modelo podemos treinar o LightGBM em todos os dados e realizar a previsão finnal do desafio para o mês de Agosto:
```{python, eval=F, echo=T}
if FINAL_PREDICTION:
    print("Training on all data...")
    # # Final Prediction
    print("Splitting train and test sets...")
    for train_idx, test_idx in GroupTimeSeriesSplit().split(df, groups=df["id"]):
        train_x, test_x = df.drop("TPV_t1", axis="columns").values[train_idx], df.drop("TPV_t1", axis="columns").values[test_idx]
        train_y, test_y = df["TPV_t1"][train_idx], df["TPV_t1"][test_idx]
    print("Beginning training...")
    train_data = lgb.Dataset(train_x, label=train_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    test_data = lgb.Dataset(test_x, label=test_y, categorical_feature=categorical_vars, feature_name=x_colnames)
    lgbm_model = lgb.train(best_params, train_data, 250, valid_sets=test_data, early_stopping_rounds=50, verbose_eval=10)

    # # Predicting on Test Data
    # Creating validation sets for each month
    y_pred_lgb_final = lgbm_model.predict(df_prediction_august, num_iteration=lgbm_model.best_iteration)

    # # Exporting final results
    y_pred_lgb_final = pd.DataFrame(y_pred_lgb_final, columns=["TPV Agosto"])
    y_pred_lgb_final.to_csv(f"{data_dir}/data/predictions/prediction_august.csv")
```

## Métricas de Performance

Esta seção tem por objetivo explorar um pouco os resultados da performance do modelo na base de teste. Pretendo explorar como varia a performance do modelo de acordo com o setor ou região do cliente. Também desejo comprovar a superioridade dos resultados de meu modelo em relação à média histórica do TPV.
\
\
Vamos começar importando os pacotes, a fonte de texto que iremos utilizar para os gráficos, e os dataframes com as métricas de performance:
```{r, echo=T, eval=T}
library(tidyverse)
library(showtext)
library(Metrics)
library(grid)
library(ggcharts)
library(mdthemes)

## Importing font
font_add(family = "Sharon", regular = "visualization/fonts/Sharon.ttf")
showtext_auto()

### Reading dataframes
df_perf = read_csv("data/model/performance/df_performance.csv") %>% 
  select(-1)
df_tpvs = read_csv("data/clean/stone_data_cleaned.csv") 
```
\
Primeiramente, vou juntar a base de dados cadastrais e de TPVs com a de performance. Tive que realizar algumas pivotagens e manipulações para isso, como pode ser observado no código abaixo:
```{r, echo=T, eval=T}
### Getting last five rows for each id
df_tpvs = df_tpvs %>% 
  arrange(id, mes_referencia) %>% 
  group_by(id) %>% 
  slice_tail(n=5) %>% 
  select(-mes_referencia, -TPV_mensal) %>% 
  unique()

### Pivoting dataframes for merge
df_tpv_real = df_perf %>% 
  pivot_longer(starts_with("TPV_t"), values_to="TPV_mensal", names_to="lag") %>% 
  select(id, TPV_mensal, lag)

df_tpv_pred = df_perf %>% 
  pivot_longer(starts_with("TPV_pred_t"), values_to="TPV_pred", names_to="lag") %>% 
  select(TPV_pred, TPV_historic_mean)

### Merging dataframes
df_comp = bind_cols(df_tpv_pred, df_tpv_real) %>% 
  select(id, everything()) %>% 
  mutate(lag=as.numeric(str_remove(lag, "TPV_t")))
rm(df_tpv_real)
rm(df_tpv_pred)

### Transforming df_comp to long format to plot graphs
df_comp_long = df_comp %>% 
  pivot_longer(-c(id, lag), values_to="valor", names_to="tipo")
```
\
Antes de continuar, gostaria de mostrar as primeiras linhas da base que utilizarei para os gráficos. Quero ressaltar que não há entradas para alguns clientes pois tive que selecionar apenas clientes com, no mínimo, 6 meses de histórico para treinar o modelo
```{r, echo=T, eval=T}
head(df_comp, n=15)
```
\
Para começar, algo muito interessante que podemos visualizar, é a performance do modelo e da média histórica do TPV para cada cliente. Vou mostrar esses resultados para os primeiros clientes:
\
```{r, echo=F, eval=T, fig.align="left"}
### Performance do modelo e da média histórica por cliene
df_comp_long %>% 
  filter(id==1) %>% 
  ggplot(mapping=aes(x=lag, y=valor, color=tipo))+
  geom_line(size=1.2)+theme_bw()+ xlab("Dia (t+1)") + ylab("TPV (R$)")+
    theme(text=element_text(family="Sharon",size=12, color = "black"))+
    theme(axis.text.x = element_text(colour="black"), axis.text.y = element_text(colour="black"))+
    theme(panel.grid.major = element_line(size=0.1, color="black"))+
    theme(panel.grid.minor = element_line(size=0, color="black"))+
    scale_color_manual(labels=c("Média Histórica","Valor Real", "Modelo"), values=c("#48acf0", "#02111b", "#00a868")) +
    labs(color = "Legenda")+ ggtitle("Performance do Modelo (Client ID = 1)")

df_comp_long %>% 
  filter(id==3) %>% 
  ggplot(mapping=aes(x=lag, y=valor, color=tipo))+
  geom_line(size=1.2)+theme_bw()+ xlab("Dia (t+1)") + ylab("TPV (R$)")+
    theme(text=element_text(family="Sharon",size=12, color = "black"))+
    theme(axis.text.x = element_text(colour="black"), axis.text.y = element_text(colour="black"))+
    theme(panel.grid.major = element_line(size=0.1, color="black"))+
    theme(panel.grid.minor = element_line(size=0, color="black"))+
    scale_color_manual(labels=c("Média Histórica","Valor Real", "Modelo"), values=c("#48acf0", "#02111b", "#00a868")) +
    labs(color = "Legenda")+ ggtitle("Performance do Modelo (Client ID = 3)")

df_comp_long %>% 
  filter(id==4) %>% 
  ggplot(mapping=aes(x=lag, y=valor, color=tipo))+
  geom_line(size=1.2)+theme_bw()+ xlab("Dia (t+1)") + ylab("TPV (R$)")+
    theme(text=element_text(family="Sharon",size=12, color = "black"))+
    theme(axis.text.x = element_text(colour="black"), axis.text.y = element_text(colour="black"))+
    theme(panel.grid.major = element_line(size=0.1, color="black"))+
    theme(panel.grid.minor = element_line(size=0, color="black"))+
    scale_color_manual(labels=c("Média Histórica","Valor Real", "Modelo"), values=c("#48acf0", "#02111b", "#00a868")) +
    labs(color = "Legenda")+ ggtitle("Performance do Modelo (Client ID = 4)")

df_comp_long %>% 
  filter(id==5) %>% 
  ggplot(mapping=aes(x=lag, y=valor, color=tipo))+
  geom_line(size=1.2)+theme_bw()+ xlab("Dia (t+1)") + ylab("TPV (R$)")+
    theme(text=element_text(family="Sharon",size=12, color = "black"))+
    theme(axis.text.x = element_text(colour="black"), axis.text.y = element_text(colour="black"))+
    theme(panel.grid.major = element_line(size=0.1, color="black"))+
    theme(panel.grid.minor = element_line(size=0, color="black"))+
    scale_color_manual(labels=c("Média Histórica","Valor Real", "Modelo"), values=c("#48acf0", "#02111b", "#00a868")) +
    labs(color = "Legenda")+ ggtitle("Performance do Modelo (Client ID = 5)")
```
\
Podemos ver que, na maior parte dos casos, a reta de previsões de nosso modelo está mais próxima do valor real do que a média móvel, o que é um sinal muito promissor.
\
\
Agora, vamos juntar à nossa base os dados cadastrais dos clientes para investigarmos a performance do modelo em diferentes setores econômicos:
```{r, echo=T, eval=T}
### Merging performance dataframe with df_comp
df_complete = inner_join(df_comp, df_tpvs, by="id") %>% 
  select(-lag)

### Grouping by id and getting the mean MAE for each client
df_complete = df_complete %>% 
  group_by(id) %>% 
  summarise(id, MAE_model = mae(TPV_mensal, TPV_pred), MAE_mean=mae(TPV_historic_mean, TPV_mensal), 
    MacroClassificacao, Estado, segmento) %>% unique()
```
\
Agora que temos todos os dados juntos, vamos investigar a performance de nosso modelo de acordo com a Macro Classificação:
\
```{r, echo=F, eval=T, fig.align="left"}
### Mean MAE by MacroClassificacao
perf_macro = df_complete %>% 
  group_by(MacroClassificacao) %>% 
  summarise(mean_mae_model=mean(MAE_model), mean_mae_historic=mean(MAE_mean)) %>% 
  arrange(mean_mae_model) %>% 
  mutate(diff=mean_mae_historic - mean_mae_model)

order_macro = drop_na(perf_macro)
order_macro = order_macro$MacroClassificacao

perf_macro %>% 
  select(-diff) %>% 
  drop_na() %>% 
  pivot_longer(-MacroClassificacao, values_to="Mean_MAE", names_to="tipo") %>% 
  ggplot(mapping=aes(y=Mean_MAE, x=MacroClassificacao,fill=tipo, fill=tipo)) +
  geom_col(color="black") + theme_bw() + xlab("Macro Classificação\n") + ylab("\nMAE Médio (menor é melhor)") +
  theme(text=element_text(family="Sharon",size=12, color = "black")) +
  theme(axis.text.x = element_text(colour="black",size=9), axis.text.y = element_text(colour="black", size=10))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  scale_fill_manual("Legenda", labels=c("Média Histórica", "Modelo"), values=c("#48acf0", "#00a868"),)+
  ggtitle("MAE Médio do                    e da                   ") +
  scale_x_discrete(limits = order_macro) + 
  annotation_custom(textGrob(expression("Modelo"), gp = gpar(col = "#00a868", fontfamily = "Sharon", cex=1.22),just="left", x = unit(0.328, "npc"), y = unit(1.048, "npc"),hjust=0))+
  annotation_custom(textGrob(expression("Média Histórica"),
                             x = unit(0.776, "npc"), y = unit(1.048, "npc"), gp = gpar(col = "#48acf0", fontfamily = "Sharon", cex=1.20)))+
  theme(legend.position = "none") + coord_flip(clip = 'off')
```
\
Podemos observar que nosso modelo possui um erro absoluto médio bastante inferior ao da média histórica na base de teste, o que é um ótimo sinal. Também podemos ver que o setor de postos de gasolina é o que o modelo tem mais dificuldades de prever. Já o setor de serviços foi onde o modelo teve sua melhor performance.
\
\
Vamos detalhar mais a performance do modelo e investigar seu comportamento de acordo com a variável "segmento":
\
```{r, echo=F, eval=T, fig.align="left"}
perf_segmento = df_complete %>% 
  group_by(segmento) %>% 
  summarise(mean_mae_model=mean(MAE_model), mean_mae_historic=mean(MAE_mean)) %>% 
  arrange(mean_mae_model) %>% 
  mutate(diff=mean_mae_historic - mean_mae_model) %>% 
  drop_na()
order_seg = perf_segmento$segmento

perf_segmento %>% 
  select(-diff) %>% 
  pivot_longer(-segmento, values_to="Mean_MAE", names_to="tipo") %>% 
  ggplot(mapping=aes(y=Mean_MAE, x=segmento,fill=tipo, fill=tipo)) +
  geom_col(color="black", stat="identity") + theme_bw() + xlab("Segmento") + ylab("MAE  Médio  (menor é melhor)") +
  theme(text=element_text(family="Sharon",size=12, color = "black")) +
  theme(axis.text.x = element_text(colour="black",size=7), axis.text.y = element_text(colour="black", size=7))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  scale_fill_manual("Legenda", labels=c("Média Histórica", "Modelo"), values=c("#48acf0", "#00a868"),)+
  ggtitle("MAE Médio do                    e da                   ") +
  scale_x_discrete(limits = order_seg) +
  annotation_custom(textGrob(expression("Modelo"), gp = gpar(col = "#00a868", fontfamily = "Sharon", cex=1.22),just="left",
                             x = unit(0.338, "npc"), y = unit(1.046, "npc"),hjust=0))+
  annotation_custom(textGrob(expression("Média Histórica"),
                             x = unit(0.799, "npc"), y = unit(1.046, "npc"), gp = gpar(col = "#48acf0", fontfamily = "Sharon", cex=1.20)))+
  theme(legend.position = "none") + coord_flip(clip = "off")

```
\
Nessa visão menos agrupada, podemos ver que a performance do modelo é bem elevada no setor de Petshops e Veterinárias, Serviços de Beleza e Estética, e Autopeças e Serviços Autimotivos. Mais uma vez, o setor de Postos de Gasolina demonstra ser difícil de prever para o modelo, sendo superado pelo de Logística e Mobilidade. Muito provavelmente, a baixa performance em relação a esses setores tem relação com o preço dos combustíveis, uma variável que pretendo incluir no modelo caso eu prossiga no desafio.
\
\
Agora que já vimos a performance do modelo de acordo com o segmento econômico, vamos observar a performance de acordo com o estado da federação para ver se o modelo tem dificuldades particulares em algum estado:
\
```{r, echo=F, eval=T, fig.align="left"}
### Mean MAE by Estado
perf_state = df_complete %>% 
  drop_na() %>% 
  group_by(Estado) %>% 
  summarise(mean_mae_model=mean(MAE_model), mean_mae_historic=mean(MAE_mean)) %>% 
  arrange(mean_mae_model) %>% 
  mutate(diff=mean_mae_historic - mean_mae_model) 

order_est = perf_state$Estado

## Making graph
perf_state %>% 
  select(-diff) %>% 
  drop_na() %>% 
  pivot_longer(-Estado, values_to="Mean_MAE", names_to="tipo") %>% 
  ggplot(mapping=aes(y=Mean_MAE, x=Estado,fill=tipo, fill=tipo)) +
  geom_col(color="black", stat="identity") + theme_bw() + xlab("Estado") + ylab("MAE  Médio  (menor é melhor)") +
  theme(text=element_text(family="Sharon",size=12, color = "black")) +
  theme(axis.text.x = element_text(colour="black",size=7), axis.text.y = element_text(colour="black", size=10))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  scale_fill_manual("Legenda", labels=c("Média Histórica", "Modelo"), values=c("#48acf0", "#00a868"),)+
  ggtitle("MAE  Médio do                    e da                   ") +
  scale_x_discrete(limits = order_est) +
  annotation_custom(textGrob(expression("Modelo"), gp = gpar(col = "#00a868", fontfamily = "Sharon", cex=1.22),just="left",
                             x = unit(0.253, "npc"), y = unit(1.045, "npc"),hjust=0))+
  annotation_custom(textGrob(expression("Média Histórica"),
                             x = unit(0.589, "npc"), y = unit(1.045, "npc"), gp = gpar(col = "#48acf0", fontfamily = "Sharon", cex=1.20)))+
  theme(legend.position = "none") + coord_cartesian(clip = "off")

```
\
É possível observar que nosso modelo apresenta uma variabilidade muito menor em relação ao erro absoluto médio quando comparado com a performance da média histórica, um sinal muito promissor. Mesmo assim, vemos que o modelo possui uma considerável diferença de performance quando comparamos estados como o Espírito Santo e Piauí. Uma variável que poderíamos adicionar ao modelo para tentar captar essas diferenças é o número de casos de covid no último mês, porque sabemos que a pandemia não ocorre de forma homogênea no território nacional e, certamente, tem um impacto nas atividades econômicas.
\
\
Para finalizar, vamos comparar os gráficos de dispersão entre as previsões de nosso modelo com os valores reais do TPV e entre a média histórica do TPV e os valores reais:
```{r, echo=F, eval=T, fig.align="left"}
### Dispersion graph between TPV_sum and TPV_pred_sum
ggplot(data=df_perf, mapping=aes(y=TPV_sum, x=TPV_pred_sum)) + 
  geom_point(alpha=0.8, color="black") +
  xlim(0, 3000000)+
  ylim(0,2500000)+
  geom_smooth(method="lm", color="#00a868", size=1.3)+
  theme_bw() +
  theme(axis.text.x = element_text(colour="black",size=7), axis.text.y = element_text(colour="black", size=7))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  xlab("Soma do TPV Real") + ylab("Soma do TPV do Modelo")+
  ggtitle("Performance do Modelo")

### Dispersion graph between TPV_sum and TPV_mean_sum
ggplot(data=df_perf, mapping=aes(y=TPV_sum, x=TPV_mean_sum)) + 
  geom_point(alpha=0.8, color="black") +
  xlim(0, 3000000)+
  ylim(0,2500000)+
  geom_smooth(method="lm", color="#48acf0", size=1.3)+
  theme_bw()+
  theme(axis.text.x = element_text(colour="black",size=7), axis.text.y = element_text(colour="black", size=7))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  xlab("Soma do TPV Real") + ylab("Soma do TPV da  Média Histórica")+
  ggtitle("Performance Média Histórica")
```
\
Agora vamos juntar as duas linhas de regressão em um só gráfico para ver qual possui a maior inclinação:
\
```{r, echo=F, eval=T, fig.align="left"}
### comparing both performances
ggplot(data=df_perf, mapping=aes(y=TPV_sum)) + 
  xlim(0, 3000000)+
  ylim(0,2500000)+
  geom_smooth(mapping=aes(x=TPV_pred_sum), method="lm", color="#00a868", size=1.3)+
  geom_smooth(mapping=aes(x=TPV_mean_sum), method="lm", color="#48acf0", size=1.3)+
  theme_bw()+
  theme(axis.text.x = element_text(colour="black",size=7), axis.text.y = element_text(colour="black", size=7))+
  theme(panel.grid.major = element_line(size=0))+
  theme(panel.grid.minor = element_line(size=0))+
  theme(text=element_text(family="Sharon",size=12, color = "black"))+
  xlab("Soma do TPV Real") + ylab("Soma do TPV da Previsto")+
  ggtitle("Performance do                    e Performance da") +
  annotation_custom(textGrob(expression("Modelo"), gp = gpar(col = "#00a868", fontfamily = "Sharon", cex=1.22),just="left",
                             x = unit(0.267, "npc"), y = unit(1.045, "npc"),hjust=0))+
  annotation_custom(textGrob(expression("Média Histórica"),
                             x = unit(0.823, "npc"), y = unit(1.045, "npc"), gp = gpar(col = "#48acf0", fontfamily = "Sharon", cex=1.20)))+
  theme(legend.position = "none") + coord_cartesian(clip = "off")
  
```
\
Podemos ver que a inclinação da reta do modelo (verde) é, claramente superior. Demonstrando que, pelo menos, para a base de teste nosso modelo claramente supera a performance da média histórica do TPV. 

## Considerações
Gostaria de dizer que foi um grande prazer participar deste desafio. Independentemente de qualquer resultado ou métrica de performance, aprendi bastante sobre problemas de séries temporais e alguns métodos específicos a eles. Espero que o conteúdo tenha sido interessante e prazeroso para vocês, assim como foi para mim. 
